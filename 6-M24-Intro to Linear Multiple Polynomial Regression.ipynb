{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31aec744-5e97-4642-b4b0-90543fb17b8b",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "**Simple Linear Regression**: This involves one independent variable and one dependent variable. It models the relationship between the two by fitting a straight line (y = mx + c).\n",
    "\n",
    "**Example**: Predicting house prices based on square footage alone.\n",
    "\n",
    "**Multiple Linear Regression**: This involves two or more independent variables affecting the dependent variable. The model fits a hyperplane (y = b1x1 + b2x2 + ... + c).\n",
    "\n",
    "**Example**: Predicting house prices based on square footage, number of rooms, and age of the house.\n",
    "\n",
    "In simple terms, simple linear regression has one predictor, while multiple linear regression has more than one predictor for the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e2f58-728d-4e2c-8f90-2a4d1fceae6d",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "\n",
    "### Assumptions of Linear Regression\n",
    "1. **Linearity**: The relationship between the independent and dependent variables is linear.\n",
    "2. **Independence**: The residuals (errors) are independent of each other.\n",
    "3. **Homoscedasticity**: Constant variance of the residuals across all levels of the independent variables.\n",
    "4. **Normality**: The residuals are normally distributed.\n",
    "5. **No Multicollinearity**: The independent variables are not highly correlated with each other.\n",
    "\n",
    "### Checking the Assumptions\n",
    "1. **Linearity**: \n",
    "   - Check scatter plots of the independent variables against the dependent variable to observe a linear trend.\n",
    "2. **Independence**:\n",
    "   - Check the Durbin-Watson test for autocorrelation of residuals.\n",
    "3. **Homoscedasticity**:\n",
    "   - Use residual plots to check if the residuals have constant variance (spread should not increase or decrease).\n",
    "4. **Normality**:\n",
    "   - Use a Q-Q plot or histogram of residuals to check for normal distribution.\n",
    "5. **No Multicollinearity**:\n",
    "   - Check the Variance Inflation Factor (VIF). A VIF value above 10 suggests high multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed705a23-c109-4c99-b592-03a5eecfe00e",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "In a linear regression model:\n",
    "\n",
    "1. **Slope (β1)**: It represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X).\n",
    "   - **Interpretation**: If the slope is 5, for every 1 unit increase in X, Y increases by 5.\n",
    "\n",
    "2. **Intercept (β0)**: It’s the predicted value of Y when X is zero, representing the starting point.\n",
    "   - **Interpretation**: If the intercept is 10, the predicted value of Y when X is 0 is 10.\n",
    "\n",
    "**Example**: \n",
    "In a model predicting house price based on square footage:\n",
    "- Slope (500): For every 1 additional square foot, the house price increases by Rs.500.\n",
    "- Intercept (50,000): When square footage is zero, the baseline house price is Rs.50,000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b37b924-cbab-4bb2-83c9-2c5dc6677fb6",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "**Gradient Descent** is an optimization algorithm used to minimize the loss (error) function in machine learning models by iteratively adjusting parameters (weights and biases).\n",
    "\n",
    "**Concept**:\n",
    "- The algorithm starts with an initial guess for the parameters.\n",
    "- It calculates the gradient (slope) of the loss function with respect to the parameters.\n",
    "- Parameters are updated in the opposite direction of the gradient to reduce the error.\n",
    "- This process continues until the model converges to the minimum error (optimal solution).\n",
    "\n",
    "**Usage in Machine Learning**:\n",
    "- In linear regression, it is used to find the best-fitting line by minimizing the sum of squared errors between predicted and actual values.\n",
    "- In neural networks, it adjusts weights to minimize the loss during training.\n",
    "\n",
    "Gradient Descent ensures that the model improves with each iteration by reducing the error step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b63d5-5576-49f7-aa39-276ecbf340de",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "**Multiple Linear Regression Model**: It is used to predict a dependent variable based on two or more independent variables. The model equation is:\n",
    "\n",
    "\\[ Y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n \\]\n",
    "\n",
    "Where:\n",
    "- \\( Y \\) is the dependent variable.\n",
    "- \\( X_1, X_2, ..., X_n \\) are independent variables.\n",
    "- \\( b_0 \\) is the intercept.\n",
    "- \\( b_1, b_2, ..., b_n \\) are the coefficients (slopes) for each independent variable.\n",
    "\n",
    "**Difference from Simple Linear Regression**:\n",
    "- **Simple Linear Regression**: One independent variable, predicting \\( Y \\) using only one feature.\n",
    "  - Example: Predicting house price based on square footage alone.\n",
    "  \n",
    "- **Multiple Linear Regression**: Two or more independent variables, predicting \\( Y \\) using multiple features.\n",
    "  - Example: Predicting house price based on square footage, number of bedrooms, and location.\n",
    "\n",
    "In multiple linear regression, multiple factors contribute to the outcome, while simple linear regression involves just one factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770dffdc-2a97-4355-99ee-5e64039e3216",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "**Multicollinearity** occurs when two or more independent variables in a multiple linear regression model are highly correlated, meaning they provide redundant information about the dependent variable. This can make it difficult to determine the individual effect of each variable, leading to unreliable coefficient estimates and high standard errors.\n",
    "\n",
    "**How to Detect Multicollinearity**:\n",
    "1. **Variance Inflation Factor (VIF)**: VIF > 5 (or sometimes > 10) indicates high multicollinearity.\n",
    "2. **Correlation Matrix**: A correlation above 0.8 between independent variables suggests multicollinearity.\n",
    "\n",
    "**How to Address Multicollinearity**:\n",
    "1. **Remove Highly Correlated Variables**: Eliminate one of the highly correlated variables.\n",
    "2. **Principal Component Analysis (PCA)**: Reduce dimensionality by transforming the variables into principal components.\n",
    "3. **Regularization Techniques**: Use Ridge or Lasso regression, which can penalize and reduce multicollinearity effects.\n",
    "\n",
    "Detecting and addressing multicollinearity ensures that your model coefficients are reliable and interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dee17ed-4ebf-4016-9b39-72493c92d520",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "**Polynomial Regression Model**: This is an extension of linear regression that models the relationship between the independent variable(s) and the dependent variable as an nth-degree polynomial. The model equation is:\n",
    "\n",
    "\\[ Y = b_0 + b_1X + b_2X^2 + ... + b_nX^n \\]\n",
    "\n",
    "Where:\n",
    "- \\( Y \\) is the dependent variable.\n",
    "- \\( X \\) is the independent variable.\n",
    "- \\( b_0, b_1, ..., b_n \\) are the coefficients.\n",
    "- \\( n \\) is the degree of the polynomial.\n",
    "\n",
    "**Difference from Linear Regression**:\n",
    "- **Linear Regression**: Models the relationship as a straight line (y = b0 + b1X). It assumes a linear relationship between the variables.\n",
    "- **Polynomial Regression**: Models the relationship as a curve (y = b0 + b1X + b2X^2 + ... + bnX^n). It allows for more complex, non-linear relationships between the variables.\n",
    "\n",
    "Polynomial regression is useful when the data shows a non-linear pattern that linear regression cannot capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939dc7d-6e7e-45e2-b389-31f2b3ddd89c",
   "metadata": {},
   "source": [
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "**Advantages of Polynomial Regression**:\n",
    "1. **Captures Non-Linearity**: Can model more complex, non-linear relationships between variables.\n",
    "2. **Flexible**: Allows for fitting curves and capturing patterns that linear regression cannot.\n",
    "\n",
    "**Disadvantages of Polynomial Regression**:\n",
    "1. **Overfitting**: Higher-degree polynomials can lead to overfitting, where the model captures noise rather than the underlying trend.\n",
    "2. **Complexity**: More complex models can be harder to interpret and require careful tuning to avoid overfitting.\n",
    "3. **Computationally Intensive**: Higher-degree polynomials increase computational costs.\n",
    "\n",
    "**When to Prefer Polynomial Regression**:\n",
    "- **Non-Linear Data**: When the relationship between the variables is clearly non-linear and a straight line doesn’t fit the data well.\n",
    "- **Curve Fitting**: When you need to fit a smooth curve to the data for better predictions and insights.\n",
    "- **Complex Patterns**: When there are intricate patterns in the data that linear regression cannot capture.\n",
    "\n",
    "Polynomial regression is useful when you need a more flexible model to capture complex relationships, but it's important to balance model complexity with the risk of overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
